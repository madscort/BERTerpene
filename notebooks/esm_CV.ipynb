{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a55e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers package WITH the ESM model. \n",
    "# It is unfortunately not available in the official release yet.\n",
    "#!git clone -b add_esm-proper --single-branch https://github.com/liujas000/transformers.git \n",
    "#!pip -q install ./transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23d8f88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import time, datetime, warnings, torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from transformers import pipeline, ESMTokenizer, ESMForSequenceClassification, AdamW, logging\n",
    "from transformers import get_linear_schedule_with_warmup as linear_scheduler\n",
    "from transformers import get_constant_schedule_with_warmup as constant_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# Use MPS or CUDA if available:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Stop verbose transformers logging:\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Stop sklearn warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204e00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing helper function\n",
    "def time_step(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f5f3258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is all about proteins, friends and vernacular. It contains many exercises for improving the body, including:\n",
      "\n",
      "Bodybuilders want a strong, balanced body\n",
      "\n",
      "Many men only want an intense workout\n",
      "\n",
      "You are all supposed to\n"
     ]
    }
   ],
   "source": [
    "# What is this notebook about?\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\", pad_token_id = 50256, num_return_sequences=1)\n",
    "print(generator(\"This notebook is all about proteins, friends and \")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a6d461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files contain: 534 sequences in 10 main categories and 49 classes\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Get sequences, accession number and main category labels:\n",
    "sequence = \"\"\n",
    "sequences = list()\n",
    "acc_num = list()\n",
    "main_cat = list()\n",
    "\n",
    "first = True\n",
    "with open(\"../data/terp.faa\") as file:\n",
    "    \n",
    "    first_acc = file.readline()\n",
    "    acc_num.append(first_acc.split(\">\")[1].strip())\n",
    "    main_cat.append(first_acc.split(\"_\")[1].strip())\n",
    "\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            sequences.append(sequence)\n",
    "            sequence = \"\"\n",
    "            acc_num.append(line.split(\">\")[1].strip())\n",
    "            main_cat.append(line.split(\"_\")[1].strip())\n",
    "        else:\n",
    "            sequence += line.strip()\n",
    "    \n",
    "    # Add last sequence\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Create numbered labels for main categories:\n",
    "\n",
    "main2label = {c: l for l, c in enumerate(sorted(set(main_cat)))}\n",
    "label2main = {l: c for c, l in main2label.items()}\n",
    "\n",
    "# Create class translation dictionary for accession numbers:\n",
    "\n",
    "acc2class = dict()\n",
    "\n",
    "with open(\"../data/class_vs_acc_v2.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        t_class = line.split(\"\\t\")[0]\n",
    "        acc = line.split(\"\\t\")[1].strip()[1:]\n",
    "        acc2class[acc] = t_class\n",
    "\n",
    "# Create numbered labels for classes:\n",
    "        \n",
    "class2label = {c: l for l, c in enumerate(sorted(set(acc2class.values())))}\n",
    "label2class = {l: c for c, l in class2label.items()}\n",
    "\n",
    "print(\n",
    "    f\"The files contain:\",\n",
    "    f\"{len(sequences)} sequences in\",\n",
    "    f\"{len(set(main_cat))} main categories and\",\n",
    "    f\"{len(set(acc2class.values()))} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f0f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibly check class distribution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f93f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between category and class:\n",
    "labels = main_cat\n",
    "#labels = acc_num # This will translate to class later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d81780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'EsmTokenizer'. \n",
      "The class this function is called from is 'ESMTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer:\n",
    "tokenizer = ESMTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\",\n",
    "                                         do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f92aa489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_sequences, input_labels, categories=True):\n",
    "        \n",
    "        # Init is run once, when instantiating the dataset class.\n",
    "        #\n",
    "        # Either supply with:\n",
    "        #  Main categories - category classification\n",
    "        #  Accession numbers - class classifcation \n",
    "        \n",
    "        # The xx2label turns the label from text to a number from 0-(N-1) \n",
    "        if categories:\n",
    "            self.labels = [main2label[cat] for cat in input_labels]\n",
    "        else:\n",
    "            self.labels = [class2label[acc2class[acc]] for acc in input_labels]\n",
    "        \n",
    "        # Tokenize sequence and pad to longest sequence in dataset.\n",
    "        # Return pytorch-type tensors\n",
    "        self.sequences = tokenizer(\n",
    "                                input_sequences,\n",
    "                                padding = 'longest',\n",
    "                                return_tensors = 'pt')\n",
    "        # Save label type\n",
    "        self.label_type_cat = categories\n",
    "        \n",
    "    def classes(self):\n",
    "        \n",
    "        # Returns the classes in the dataset (optional function)\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        # Returns the number of samples in dataset (required)\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Returns a sample at position idx (required)\n",
    "        # The sample includes:\n",
    "        # - Input id's for the sequence\n",
    "        # - Attention mask (to only focus on the sequence and not padding)\n",
    "        # - Label (one-hot encoded)\n",
    "        \n",
    "        input_ids = self.sequences['input_ids'][idx]\n",
    "        attention_mask = self.sequences['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        num_labels = len(main2label.values()) if self.label_type_cat else len(class2label.values())\n",
    "        \n",
    "        sample = dict()\n",
    "        sample['input_ids'] = input_ids\n",
    "        sample['attention_mask'] = attention_mask\n",
    "        sample['label'] = one_hot(label,\n",
    "                                  num_classes = num_labels).to(torch.float)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90925b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS:\n",
    "\n",
    "# Model specifications:\n",
    "model_name = \"esm2_t6_8M_UR50D\"\n",
    "model_version = \"facebook/\" + model_name\n",
    "num_labels = 10\n",
    "\n",
    "# Training\n",
    "epochs = 1\n",
    "batch_size = 1\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Cross-validation\n",
    "num_splits = 5\n",
    "# # Stratified (force even distribution of classes)\n",
    "# folds = StratifiedKFold(n_splits = num_splits,\n",
    "#                         shuffle = True)\n",
    "# Don't stratify:\n",
    "folds = KFold(n_splits = num_splits,\n",
    "              shuffle = True)\n",
    "\n",
    "# Logging:\n",
    "out_name = \"_\".join([model_name,\n",
    "                     \"E\" + str(epochs),\n",
    "                     \"B\" + str(batch_size),\n",
    "                     \"CV\" + str(num_splits),\n",
    "                     \"T\" + datetime.datetime.now().strftime(\"%m%d%H%M\")])\n",
    "\n",
    "results_file = open(\"../results/validation/\" + out_name + \".res\", \"w\")\n",
    "print(f\"fold\\ttrain_loss\\tval_loss\\ttrain_accu\\tval_accu\\tbalanced_train_accu\\tbalanced_val_accu\",\n",
    "      file = results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc4ec3d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** FOLD 1 of 5 ******\n",
      "#------ EPOCH 1 of 1\n",
      "#--- Training\n",
      "#- Batch 1 of 427 \t- 0:00:02 (time elapsed)\n",
      "Average training loss: 0.00\n",
      "Average training accuracy: 0.00\n",
      "Average balanced training accuracy: 0.00\n",
      "\n",
      "#****** Validation\n",
      "Average validation loss: 0.01\n",
      "Average validation accuracy: 0.00\n",
      "Average balanced validation accuracy: 0.00\n",
      "\n",
      "****** FOLD 2 of 5 ******\n",
      "#------ EPOCH 1 of 1\n",
      "#--- Training\n",
      "#- Batch 1 of 427 \t- 0:00:04 (time elapsed)\n",
      "Average training loss: 0.00\n",
      "Average training accuracy: 0.00\n",
      "Average balanced training accuracy: 0.00\n",
      "\n",
      "#****** Validation\n",
      "Average validation loss: 0.01\n",
      "Average validation accuracy: 0.00\n",
      "Average balanced validation accuracy: 0.00\n",
      "\n",
      "****** FOLD 3 of 5 ******\n",
      "#------ EPOCH 1 of 1\n",
      "#--- Training\n",
      "#- Batch 1 of 427 \t- 0:00:06 (time elapsed)\n",
      "Average training loss: 0.00\n",
      "Average training accuracy: 0.00\n",
      "Average balanced training accuracy: 0.00\n",
      "\n",
      "#****** Validation\n",
      "Average validation loss: 0.01\n",
      "Average validation accuracy: 0.00\n",
      "Average balanced validation accuracy: 0.00\n",
      "\n",
      "****** FOLD 4 of 5 ******\n",
      "#------ EPOCH 1 of 1\n",
      "#--- Training\n",
      "#- Batch 1 of 427 \t- 0:00:08 (time elapsed)\n",
      "Average training loss: 0.00\n",
      "Average training accuracy: 0.00\n",
      "Average balanced training accuracy: 0.00\n",
      "\n",
      "#****** Validation\n",
      "Average validation loss: 0.01\n",
      "Average validation accuracy: 0.00\n",
      "Average balanced validation accuracy: 0.00\n",
      "\n",
      "****** FOLD 5 of 5 ******\n",
      "#------ EPOCH 1 of 1\n",
      "#--- Training\n",
      "#- Batch 1 of 428 \t- 0:00:10 (time elapsed)\n",
      "Average training loss: 0.00\n",
      "Average training accuracy: 0.00\n",
      "Average balanced training accuracy: 0.00\n",
      "\n",
      "#****** Validation\n",
      "Average validation loss: 0.01\n",
      "Average validation accuracy: 0.00\n",
      "Average balanced validation accuracy: 0.00\n",
      "Success!\n",
      "\n",
      "Total training time: 0:00:11\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(folds.split(sequences, labels)):\n",
    "    print(f\"\\n****** FOLD {fold+1} of {num_splits} ******\")\n",
    "\n",
    "    # Divide data (kfold wants arrays, the tokenizer wants lists = crappy looking code):\n",
    "    train_seq, train_labels = np.array(sequences)[train_index].tolist(), np.array(labels)[train_index].tolist()\n",
    "    val_seq, val_labels = np.array(sequences)[val_index].tolist(), np.array(labels)[val_index].tolist()\n",
    "\n",
    "    # Convert to SequenceDataset:\n",
    "    train_dataset = SequenceDataset(train_seq, train_labels)\n",
    "    val_dataset = SequenceDataset(val_seq, val_labels)\n",
    "\n",
    "    # Load:\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True)\n",
    "\n",
    "    # Reset model:\n",
    "    #num_labels = len(set(train_dataset.classes()))\n",
    "    model = ESMForSequenceClassification.from_pretrained(\n",
    "        model_version,\n",
    "        num_labels = num_labels,\n",
    "        problem_type = \"multi_label_classification\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Add optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = learning_rate)\n",
    "    scheduler = linear_scheduler(optimizer,\n",
    "                                 num_warmup_steps = 0,\n",
    "                                 num_training_steps = len(train_loader) * epochs)\n",
    "\n",
    "    # If you don't want decay - uncomment this:\n",
    "    # scheduler = constant_scheduler(optimizer,\n",
    "    #                                num_warmup_steps = 0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"#------ EPOCH {epoch+1} of {epochs}\")\n",
    "        print(f\"#--- Training\")\n",
    "        # Training step\n",
    "\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_accuracy = 0\n",
    "        total_balanced_train_accuracy = 0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "\n",
    "            if step % 5 == 0:\n",
    "                time_elapsed = time_step(time.time() - total_t0)\n",
    "                print(f\"#- Batch {step+1} of {len(train_loader)} \\t- {time_elapsed} (time elapsed)\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            input_labels = batch['label'].to(device)\n",
    "\n",
    "            output = model(input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=attention_mask,\n",
    "                           labels=input_labels)\n",
    "            # Get loss\n",
    "            total_train_loss += output.loss\n",
    "\n",
    "            # Get accuracy\n",
    "            logits = output.logits.detach().cpu().numpy()\n",
    "            input_labels = input_labels.to('cpu').numpy()\n",
    "            total_train_accuracy += accuracy_score(input_labels.argmax(axis=1),\n",
    "                                                 logits.argmax(axis=1))\n",
    "\n",
    "            total_balanced_train_accuracy += balanced_accuracy_score(\n",
    "                input_labels.argmax(axis=1),\n",
    "                logits.argmax(axis=1))\n",
    "\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() # Update learning rate\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_train_accuracy = total_train_accuracy / len(train_loader)\n",
    "        avg_balanced_train_accuracy = total_balanced_train_accuracy / len(train_loader)\n",
    "\n",
    "        print(f\"Average training loss: {avg_train_loss:.2f}\")\n",
    "        print(f\"Average training accuracy: {avg_train_accuracy:.2f}\")\n",
    "        print(f\"Average balanced training accuracy: {avg_balanced_train_accuracy:.2f}\")\n",
    "\n",
    "    # Validation\n",
    "    print(f\"\\n#****** Validation\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_accuracy = 0\n",
    "    total_balanced_val_accuracy = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        input_labels = batch['label'].to(device)\n",
    "\n",
    "        # Dont construct unnecessary compute-graph:\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=attention_mask,\n",
    "                           labels=input_labels)\n",
    "\n",
    "        total_val_loss += output.loss\n",
    "\n",
    "        logits = output.logits.detach().cpu().numpy()\n",
    "        input_labels = input_labels.to('cpu').numpy()\n",
    "\n",
    "        total_val_accuracy += accuracy_score(input_labels.argmax(axis=1),\n",
    "                                             logits.argmax(axis=1))\n",
    "\n",
    "        total_balanced_val_accuracy += balanced_accuracy_score(\n",
    "            input_labels.argmax(axis=1),\n",
    "            logits.argmax(axis=1))\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_val_accuracy / len(val_loader)\n",
    "    avg_balanced_val_accuracy = total_balanced_val_accuracy / len(val_loader)\n",
    "\n",
    "    print(f\"Average validation loss: {avg_val_loss:.2f}\")\n",
    "    print(f\"Average validation accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"Average balanced validation accuracy: {avg_balanced_val_accuracy:.2f}\")\n",
    "\n",
    "    # Save output:\n",
    "    print(fold + 1,\n",
    "          f\"{avg_train_loss:.4f}\",\n",
    "          f\"{avg_val_loss:.4f}\",\n",
    "          f\"{avg_train_accuracy:.4f}\",\n",
    "          f\"{avg_val_accuracy:.4f},\"\n",
    "          f\"{avg_balanced_train_accuracy:.4f}\",\n",
    "          f\"{avg_balanced_val_accuracy:.4f}\",\n",
    "          sep = \"\\t\",\n",
    "          file = results_file)\n",
    "\n",
    "print(\"Success!\\n\")\n",
    "print(f\"Total training time: {time_step(time.time() - total_t0)}\")\n",
    "results_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
