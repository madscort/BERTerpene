{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d338b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers package WITH the ESM model. \n",
    "# It is unfortunately not available in the official release yet.\n",
    "#!git clone -b add_esm-proper --single-branch https://github.com/liujas000/transformers.git \n",
    "!pip -q install ./transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "23d8f88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, ESMForTokenClassification, ESMTokenizer, ESMForMaskedLM, ESMForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# Use MPS or CUDA if available:\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f5f3258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is all about proteins, friends and ersatz \"funtime.\" How much time do we have to discuss all this protein stuff? Is it easy? How to make the best meat, a better dinner, and a better life for yourself\n"
     ]
    }
   ],
   "source": [
    "# What is this notebook about?\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\", pad_token_id = 50256, num_return_sequences=1)\n",
    "print(generator(\"This notebook is all about proteins, friends and \")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cc34dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files contain: 534 sequences in 10 main categories and 49 classes\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Get sequences, accession number and main category labels:\n",
    "\n",
    "sequence = \"\"\n",
    "sequences = list()\n",
    "acc_num = list()\n",
    "main_cat = list()\n",
    "\n",
    "first = True\n",
    "with open(\"../data/terp.faa\") as file:\n",
    "    \n",
    "    first_acc = file.readline()\n",
    "    acc_num.append(first_acc.split(\">\")[1].strip())\n",
    "    main_cat.append(first_acc.split(\"_\")[1].strip())\n",
    "\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            sequences.append(sequence)\n",
    "            sequence = \"\"\n",
    "            acc_num.append(line.split(\">\")[1].strip())\n",
    "            main_cat.append(line.split(\"_\")[1].strip())\n",
    "        else:\n",
    "            sequence += line.strip()\n",
    "    \n",
    "    # Add last sequence\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Create numbered labels for main categories:\n",
    "\n",
    "main2label = {c: l for l, c in enumerate(sorted(set(main_cat)))}\n",
    "label2main = {l: c for c, l in main2label.items()}\n",
    "\n",
    "# Create class translation dictionary for accession numbers:\n",
    "\n",
    "acc2class = dict()\n",
    "\n",
    "with open(\"../data/class_vs_acc_v2.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        t_class = line.split(\"\\t\")[0]\n",
    "        acc = line.split(\"\\t\")[1].strip()[1:]\n",
    "        acc2class[acc] = t_class\n",
    "\n",
    "# Create numbered labels for classes:\n",
    "        \n",
    "class2label = {c: l for l, c in enumerate(sorted(set(acc2class.values())))}\n",
    "label2class = {l: c for c, l in class2label.items()}\n",
    "\n",
    "print(\n",
    "    f\"The files contain:\",\n",
    "    f\"{len(sequences)} sequences in\",\n",
    "    f\"{len(set(main_cat))} main categories and\",\n",
    "    f\"{len(set(acc2class.values()))} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ac9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibly check class distribution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d94a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 480 Validation size: 54\n"
     ]
    }
   ],
   "source": [
    "# Choose between category and class:\n",
    "labels = main_cat\n",
    "#labels = acc_num # This will translate to class later.\n",
    "\n",
    "# Split into training and validation set. Is this necessary?\n",
    "\n",
    "train_seq, val_seq, train_labels, val_labels = train_test_split(sequences, labels, test_size=.1)\n",
    "\n",
    "print(f\"Training size: {len(train_seq)} Validation size: {len(val_seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4f21d4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984c650d0711435dbb39c91a636e594c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33952171ef0448c9a8bd1384009cb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c313b0dd71441aabda78c2d259ed06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'EsmTokenizer'. \n",
      "The class this function is called from is 'ESMTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer:\n",
    "tokenizer = ESMTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4dffb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_sequences, input_labels, categories=True):\n",
    "        \n",
    "        # Init is run once, when instantiating the dataset class.\n",
    "        #\n",
    "        # Either supply with:\n",
    "        #  Main categories - category classification\n",
    "        #  Accession numbers - class classifcation \n",
    "        \n",
    "        # The xx2label turns the label from text to a number from 0-(N-1) \n",
    "        if categories:\n",
    "            self.labels = [main2label[cat] for cat in input_labels]\n",
    "        else:\n",
    "            self.labels = [class2label[acc2class[acc]] for acc in input_labels]\n",
    "        \n",
    "        # Tokenize sequence and pad to longest sequence in dataset.\n",
    "        # Return pytorch-type tensors\n",
    "        self.sequences = tokenizer(\n",
    "                                input_sequences,\n",
    "                                padding = 'longest',\n",
    "                                return_tensors = 'pt')\n",
    "        # Save label type\n",
    "        self.label_type_cat = categories\n",
    "        \n",
    "    def classes(self):\n",
    "        \n",
    "        # Returns the classes in the dataset (optional function)\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        # Returns the number of samples in dataset (required)\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Returns a sample at position idx (required)\n",
    "        # The sample includes:\n",
    "        # - Input id's for the sequence\n",
    "        # - Attention mask (to only focus on the sequence and not padding)\n",
    "        # - Label (one-hot encoded)\n",
    "        \n",
    "        input_ids = self.sequences['input_ids'][idx]\n",
    "        attention_mask = self.sequences['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        num_labels = len(main2label.values()) if self.label_type_cat else len(class2label.values())\n",
    "        \n",
    "        sample = dict()\n",
    "        sample['input_ids'] = input_ids\n",
    "        sample['attention_mask'] = attention_mask\n",
    "        sample['label'] = one_hot(label,\n",
    "                                  num_classes=num_labels).to(torch.float)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1dd2f86d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(train_seq, train_labels)\n",
    "val_dataset = SequenceDataset(val_seq, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "598bd6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 0, 20, 14, 18,  5,  8, 11,  6, 23, 17, 14,  6,  4,  9, 15, 11, 10,  9,\n",
       "          5,  5, 22,  9, 22,  5,  9,  5,  9,  6,  4, 12,  4,  8,  7, 14,  5, 10,\n",
       "         10, 15, 20, 12, 10, 11, 10, 14,  9,  4, 22, 12,  8,  4, 12, 18, 14, 15,\n",
       "          5,  8, 16, 21, 21,  4, 13,  4, 18, 23, 16, 22,  4, 18, 22,  5, 18,  4,\n",
       "          7, 13, 13,  9, 18, 13, 13,  6, 14,  5,  6, 10, 13, 14,  4, 20, 23,  9,\n",
       "          5,  5, 12, 11, 10,  4,  7, 13,  7, 18, 13,  6,  5,  5, 14, 21,  6, 14,\n",
       "         20,  9, 16,  5,  4, 11,  6,  4, 10,  9, 10, 11, 23, 10, 13, 10,  8, 14,\n",
       "         16, 22, 17, 10, 16, 18, 10, 10, 13, 11,  5,  5, 22,  4, 22, 11, 19, 19,\n",
       "          5,  9,  5,  7,  9, 10,  5,  5,  6, 16,  7, 14,  8, 10,  7, 13, 18,  7,\n",
       "         15, 21, 10, 10, 13,  8,  7,  5, 20, 16, 14, 18,  4, 13,  4, 21,  9, 12,\n",
       "         11,  5,  6, 12, 13,  4, 14, 13,  8,  5, 10,  8,  4, 14,  5, 19, 12,  5,\n",
       "          4, 10, 17,  5,  7, 11, 13, 21,  8,  6,  4, 23, 17, 13, 12, 23,  8, 18,\n",
       "          9, 15,  9,  5,  5,  4,  6, 19,  9, 21, 17,  5,  7, 10,  4, 12, 16, 10,\n",
       "         13, 10, 10,  8, 11,  4, 16,  9,  5,  7, 13,  9,  5,  6, 12, 16,  4,  5,\n",
       "         10, 12,  5,  9, 10,  7,  7, 10,  5,  9, 15,  9,  4, 12,  9,  9, 12, 13,\n",
       "          5,  5,  6, 12,  8,  8,  8, 11, 10,  5,  5,  4,  9, 10, 23,  7, 16, 13,\n",
       "         19, 10,  6,  4,  7, 10,  6, 13, 18, 13, 19, 21,  5, 10,  5,  9, 10, 19,\n",
       "         11, 10, 14, 13,  4,  7,  9,  4, 13,  5, 10, 17,  8, 20,  8, 16, 19, 18,\n",
       "          5,  5,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of returning a single sample:\n",
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5b5b8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cffda873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing ESMForSequenceClassification: ['esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing ESMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ESMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ESMForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.embeddings.token_type_embeddings.weight', 'esm.embeddings.LayerNorm.weight', 'classifier.out_proj.bias', 'esm.embeddings.LayerNorm.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "# Training using pytorch only\n",
    "\n",
    "num_labels = len(set(train_dataset.classes()))\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "model = ESMForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    num_labels = num_labels,\n",
    "    problem_type = \"multi_label_classification\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Train loader returns an iter with the number of samples = batch size.\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "n = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        n += 1\n",
    "\n",
    "print(n)\n",
    "\n",
    "sys.exit(1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        input_ids = sample['input_ids'].to(device)\n",
    "        attention_mask = sample['attention_mask'].to(device)\n",
    "        label = sample['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask = attention_mask, labels=label)\n",
    "        \n",
    "        print(outputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ead1524c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.5423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.4787, -0.0347, -0.4638, -0.2717, -0.4413, -0.3257, -0.5093, -0.1469,\n",
      "         -0.5806, -0.4681]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n",
    "print(outputs[1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5da0b6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['label'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ESMTokenizer.from_pretrained(\"facebook/esm-1b\", do_lower_case=False)\n",
    "model = ESMForMaskedLM.from_pretrained(\"facebook/esm-1b\")\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "unmasker('QERLKSIVRILE<mask>SLGYNIVAT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
