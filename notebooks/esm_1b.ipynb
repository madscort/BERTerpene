{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a55e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers package WITH the ESM model. \n",
    "# It is unfortunately not available in the official release yet.\n",
    "#!git clone -b add_esm-proper --single-branch https://github.com/liujas000/transformers.git \n",
    "#!pip -q install ./transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d8f88d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from transformers import pipeline, ESMTokenizer, ESMForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup as linear_scheduler\n",
    "from transformers import get_constant_schedule_with_warmup as constant_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "# Use MPS or CUDA if available:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204e00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing helper function\n",
    "def time_step(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5f3258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook is all about proteins, friends and vernaculars. You won't want to look around the kitchen to find out where someone's egg salad was made, you won't want to know where your chicken salad is made, or that the\n"
     ]
    }
   ],
   "source": [
    "# What is this notebook about?\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\", pad_token_id = 50256, num_return_sequences=1)\n",
    "print(generator(\"This notebook is all about proteins, friends and \")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6d461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files contain: 534 sequences in 10 main categories and 49 classes\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Get sequences, accession number and main category labels:\n",
    "\n",
    "sequence = \"\"\n",
    "sequences = list()\n",
    "acc_num = list()\n",
    "main_cat = list()\n",
    "\n",
    "first = True\n",
    "with open(\"../data/terp.faa\") as file:\n",
    "    \n",
    "    first_acc = file.readline()\n",
    "    acc_num.append(first_acc.split(\">\")[1].strip())\n",
    "    main_cat.append(first_acc.split(\"_\")[1].strip())\n",
    "\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            sequences.append(sequence)\n",
    "            sequence = \"\"\n",
    "            acc_num.append(line.split(\">\")[1].strip())\n",
    "            main_cat.append(line.split(\"_\")[1].strip())\n",
    "        else:\n",
    "            sequence += line.strip()\n",
    "    \n",
    "    # Add last sequence\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Create numbered labels for main categories:\n",
    "\n",
    "main2label = {c: l for l, c in enumerate(sorted(set(main_cat)))}\n",
    "label2main = {l: c for c, l in main2label.items()}\n",
    "\n",
    "# Create class translation dictionary for accession numbers:\n",
    "\n",
    "acc2class = dict()\n",
    "\n",
    "with open(\"../data/class_vs_acc_v2.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        t_class = line.split(\"\\t\")[0]\n",
    "        acc = line.split(\"\\t\")[1].strip()[1:]\n",
    "        acc2class[acc] = t_class\n",
    "\n",
    "# Create numbered labels for classes:\n",
    "        \n",
    "class2label = {c: l for l, c in enumerate(sorted(set(acc2class.values())))}\n",
    "label2class = {l: c for c, l in class2label.items()}\n",
    "\n",
    "print(\n",
    "    f\"The files contain:\",\n",
    "    f\"{len(sequences)} sequences in\",\n",
    "    f\"{len(set(main_cat))} main categories and\",\n",
    "    f\"{len(set(acc2class.values()))} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f0f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibly check class distribution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f93f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 480 Validation size: 54\n"
     ]
    }
   ],
   "source": [
    "# Choose between category and class:\n",
    "labels = main_cat\n",
    "#labels = acc_num # This will translate to class later.\n",
    "\n",
    "# Split into training and validation set. Is this necessary?\n",
    "\n",
    "train_seq, val_seq, train_labels, val_labels = train_test_split(sequences, labels, test_size=.1)\n",
    "\n",
    "print(f\"Training size: {len(train_seq)} Validation size: {len(val_seq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d81780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'EsmTokenizer'. \n",
      "The class this function is called from is 'ESMTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer:\n",
    "tokenizer = ESMTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\",\n",
    "                                         do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92aa489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, input_sequences, input_labels, categories=True):\n",
    "        \n",
    "        # Init is run once, when instantiating the dataset class.\n",
    "        #\n",
    "        # Either supply with:\n",
    "        #  Main categories - category classification\n",
    "        #  Accession numbers - class classifcation \n",
    "        \n",
    "        # The xx2label turns the label from text to a number from 0-(N-1) \n",
    "        if categories:\n",
    "            self.labels = [main2label[cat] for cat in input_labels]\n",
    "        else:\n",
    "            self.labels = [class2label[acc2class[acc]] for acc in input_labels]\n",
    "        \n",
    "        # Tokenize sequence and pad to longest sequence in dataset.\n",
    "        # Return pytorch-type tensors\n",
    "        self.sequences = tokenizer(\n",
    "                                input_sequences,\n",
    "                                padding = 'longest',\n",
    "                                return_tensors = 'pt')\n",
    "        # Save label type\n",
    "        self.label_type_cat = categories\n",
    "        \n",
    "    def classes(self):\n",
    "        \n",
    "        # Returns the classes in the dataset (optional function)\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        # Returns the number of samples in dataset (required)\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Returns a sample at position idx (required)\n",
    "        # The sample includes:\n",
    "        # - Input id's for the sequence\n",
    "        # - Attention mask (to only focus on the sequence and not padding)\n",
    "        # - Label (one-hot encoded)\n",
    "        \n",
    "        input_ids = self.sequences['input_ids'][idx]\n",
    "        attention_mask = self.sequences['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        num_labels = len(main2label.values()) if self.label_type_cat else len(class2label.values())\n",
    "        \n",
    "        sample = dict()\n",
    "        sample['input_ids'] = input_ids\n",
    "        sample['attention_mask'] = attention_mask\n",
    "        sample['label'] = one_hot(label,\n",
    "                                  num_classes = num_labels).to(torch.float)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983826ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to dataset class:\n",
    "train_dataset = SequenceDataset(train_seq, train_labels)\n",
    "val_dataset = SequenceDataset(val_seq, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90925b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing ESMForSequenceClassification: ['esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'lm_head.dense.bias', 'lm_head.bias', 'esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq', 'lm_head.layer_norm.weight', 'esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq']\n",
      "- This IS expected if you are initializing ESMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ESMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ESMForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.weight', 'esm.embeddings.token_type_embeddings.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'esm.embeddings.LayerNorm.bias', 'esm.embeddings.LayerNorm.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS:\n",
    "\n",
    "# Model specifications:\n",
    "num_labels = len(set(train_dataset.classes()))\n",
    "model = ESMForSequenceClassification.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    num_labels = num_labels,\n",
    "    problem_type = \"multi_label_classification\")\n",
    "\n",
    "# Data loading:\n",
    "# The DataLoader splits the dataset by batch size,\n",
    "# and returns an iter to go through each batch of samples.\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "# Optimizer, learning rate and optional learning rate decay.\n",
    "\n",
    "learning_rate = 5e-5\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = learning_rate)\n",
    "scheduler = linear_scheduler(optimizer,\n",
    "                             num_warmup_steps = 0,\n",
    "                             num_training_steps = len(train_loader) * epochs)\n",
    "\n",
    "# If you don't want decay - uncomment this:\n",
    "# scheduler = constant_scheduler(optimizer,\n",
    "#                                num_warmup_steps = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc4ec3d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------ EPOCH 1 of 1\n",
      "\n",
      "Training\n",
      "\n",
      "#--- Batch 1 of 480\n",
      "Time: 0:00:00\n",
      "#--- Batch 6 of 480\n",
      "Time: 0:00:04\n",
      "#--- Batch 11 of 480\n",
      "Time: 0:00:08\n",
      "#--- Batch 16 of 480\n",
      "Time: 0:00:11\n",
      "#--- Batch 21 of 480\n",
      "Time: 0:00:15\n",
      "#--- Batch 26 of 480\n",
      "Time: 0:00:18\n",
      "#--- Batch 31 of 480\n",
      "Time: 0:00:22\n",
      "#--- Batch 36 of 480\n",
      "Time: 0:00:25\n",
      "#--- Batch 41 of 480\n",
      "Time: 0:00:29\n",
      "#--- Batch 46 of 480\n",
      "Time: 0:00:32\n",
      "#--- Batch 51 of 480\n",
      "Time: 0:00:35\n",
      "#--- Batch 56 of 480\n",
      "Time: 0:00:39\n",
      "#--- Batch 61 of 480\n",
      "Time: 0:00:42\n",
      "#--- Batch 66 of 480\n",
      "Time: 0:00:46\n",
      "#--- Batch 71 of 480\n",
      "Time: 0:00:49\n",
      "#--- Batch 76 of 480\n",
      "Time: 0:00:53\n",
      "#--- Batch 81 of 480\n",
      "Time: 0:00:56\n",
      "#--- Batch 86 of 480\n",
      "Time: 0:01:00\n",
      "#--- Batch 91 of 480\n",
      "Time: 0:01:03\n",
      "#--- Batch 96 of 480\n",
      "Time: 0:01:07\n",
      "#--- Batch 101 of 480\n",
      "Time: 0:01:11\n",
      "#--- Batch 106 of 480\n",
      "Time: 0:01:14\n",
      "#--- Batch 111 of 480\n",
      "Time: 0:01:18\n",
      "#--- Batch 116 of 480\n",
      "Time: 0:01:21\n",
      "#--- Batch 121 of 480\n",
      "Time: 0:01:25\n",
      "#--- Batch 126 of 480\n",
      "Time: 0:01:29\n",
      "#--- Batch 131 of 480\n",
      "Time: 0:01:32\n",
      "#--- Batch 136 of 480\n",
      "Time: 0:01:36\n",
      "#--- Batch 141 of 480\n",
      "Time: 0:01:39\n",
      "#--- Batch 146 of 480\n",
      "Time: 0:01:43\n",
      "#--- Batch 151 of 480\n",
      "Time: 0:01:46\n",
      "#--- Batch 156 of 480\n",
      "Time: 0:01:50\n",
      "#--- Batch 161 of 480\n",
      "Time: 0:01:53\n",
      "#--- Batch 166 of 480\n",
      "Time: 0:01:57\n",
      "#--- Batch 171 of 480\n",
      "Time: 0:02:00\n",
      "#--- Batch 176 of 480\n",
      "Time: 0:02:03\n",
      "#--- Batch 181 of 480\n",
      "Time: 0:02:07\n",
      "#--- Batch 186 of 480\n",
      "Time: 0:02:11\n",
      "#--- Batch 191 of 480\n",
      "Time: 0:02:15\n",
      "#--- Batch 196 of 480\n",
      "Time: 0:02:19\n",
      "#--- Batch 201 of 480\n",
      "Time: 0:02:22\n",
      "#--- Batch 206 of 480\n",
      "Time: 0:02:26\n",
      "#--- Batch 211 of 480\n",
      "Time: 0:02:29\n",
      "#--- Batch 216 of 480\n",
      "Time: 0:02:32\n",
      "#--- Batch 221 of 480\n",
      "Time: 0:02:36\n",
      "#--- Batch 226 of 480\n",
      "Time: 0:02:39\n",
      "#--- Batch 231 of 480\n",
      "Time: 0:02:43\n",
      "#--- Batch 236 of 480\n",
      "Time: 0:02:46\n",
      "#--- Batch 241 of 480\n",
      "Time: 0:02:50\n",
      "#--- Batch 246 of 480\n",
      "Time: 0:02:53\n",
      "#--- Batch 251 of 480\n",
      "Time: 0:02:57\n",
      "#--- Batch 256 of 480\n",
      "Time: 0:03:00\n",
      "#--- Batch 261 of 480\n",
      "Time: 0:03:04\n",
      "#--- Batch 266 of 480\n",
      "Time: 0:03:07\n",
      "#--- Batch 271 of 480\n",
      "Time: 0:03:11\n",
      "#--- Batch 276 of 480\n",
      "Time: 0:03:14\n",
      "#--- Batch 281 of 480\n",
      "Time: 0:03:17\n",
      "#--- Batch 286 of 480\n",
      "Time: 0:03:21\n",
      "#--- Batch 291 of 480\n",
      "Time: 0:03:24\n",
      "#--- Batch 296 of 480\n",
      "Time: 0:03:28\n",
      "#--- Batch 301 of 480\n",
      "Time: 0:03:31\n",
      "#--- Batch 306 of 480\n",
      "Time: 0:03:35\n",
      "#--- Batch 311 of 480\n",
      "Time: 0:03:38\n",
      "#--- Batch 316 of 480\n",
      "Time: 0:03:42\n",
      "#--- Batch 321 of 480\n",
      "Time: 0:03:47\n",
      "#--- Batch 326 of 480\n",
      "Time: 0:03:51\n",
      "#--- Batch 331 of 480\n",
      "Time: 0:03:54\n",
      "#--- Batch 336 of 480\n",
      "Time: 0:03:58\n",
      "#--- Batch 341 of 480\n",
      "Time: 0:04:02\n",
      "#--- Batch 346 of 480\n",
      "Time: 0:04:06\n",
      "#--- Batch 351 of 480\n",
      "Time: 0:04:10\n",
      "#--- Batch 356 of 480\n",
      "Time: 0:04:15\n",
      "#--- Batch 361 of 480\n",
      "Time: 0:04:20\n",
      "#--- Batch 366 of 480\n",
      "Time: 0:04:24\n",
      "#--- Batch 371 of 480\n",
      "Time: 0:04:27\n",
      "#--- Batch 376 of 480\n",
      "Time: 0:04:31\n",
      "#--- Batch 381 of 480\n",
      "Time: 0:04:35\n",
      "#--- Batch 386 of 480\n",
      "Time: 0:04:38\n",
      "#--- Batch 391 of 480\n",
      "Time: 0:04:42\n",
      "#--- Batch 396 of 480\n",
      "Time: 0:04:45\n",
      "#--- Batch 401 of 480\n",
      "Time: 0:04:49\n",
      "#--- Batch 406 of 480\n",
      "Time: 0:04:52\n",
      "#--- Batch 411 of 480\n",
      "Time: 0:04:56\n",
      "#--- Batch 416 of 480\n",
      "Time: 0:04:59\n",
      "#--- Batch 421 of 480\n",
      "Time: 0:05:02\n",
      "#--- Batch 426 of 480\n",
      "Time: 0:05:06\n",
      "#--- Batch 431 of 480\n",
      "Time: 0:05:09\n",
      "#--- Batch 436 of 480\n",
      "Time: 0:05:13\n",
      "#--- Batch 441 of 480\n",
      "Time: 0:05:16\n",
      "#--- Batch 446 of 480\n",
      "Time: 0:05:19\n",
      "#--- Batch 451 of 480\n",
      "Time: 0:05:23\n",
      "#--- Batch 456 of 480\n",
      "Time: 0:05:26\n",
      "#--- Batch 461 of 480\n",
      "Time: 0:05:29\n",
      "#--- Batch 466 of 480\n",
      "Time: 0:05:33\n",
      "#--- Batch 471 of 480\n",
      "Time: 0:05:36\n",
      "#--- Batch 476 of 480\n",
      "Time: 0:05:39\n",
      "Average training loss: 0.28\n",
      "\n",
      "Validating\n",
      "\n",
      "Average validation loss: 0.00\n",
      "Average validation accuracy: 0.00\n",
      "Average balanced validation accuracy: 0.00\n",
      "Validation time: 0:00:00\n",
      "\n",
      "Success!\n",
      "\n",
      "Total training time: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madsniels/miniconda3/envs/hpc/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1987: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print(f\"#------ EPOCH {epoch+1} of {epochs}\")\n",
    "    print(f\"\\nTraining\\n\")\n",
    "    # Training step\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            \n",
    "            time_elapsed = time_step(time.time() - t0)\n",
    "            print(f\"#--- Batch {step+1} of {len(train_loader)}\\nTime: {time_elapsed}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        output = model(input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=attention_mask,\n",
    "                       labels=labels)\n",
    "\n",
    "        total_train_loss += output.loss\n",
    "\n",
    "        output.loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # Update learning rate\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # Evaluation step\n",
    "    \n",
    "    print(f\"\\nValidating\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    t0 = time.time()\n",
    "    total_val_loss = 0\n",
    "    total_val_accuracy = 0\n",
    "    total_balanced_val_accuracy = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Dont construct unnecessary compute-graph:\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=attention_mask,\n",
    "                           labels=labels)\n",
    "        \n",
    "        total_val_loss += output.loss\n",
    "        \n",
    "        logits = output.logits.detach().cpu().numpy()\n",
    "        labels = labels.to('cpu').numpy()\n",
    "        \n",
    "\n",
    "        \n",
    "        total_val_accuracy += accuracy_score(labels.argmax(axis=1),\n",
    "                                             logits.argmax(axis=1))\n",
    "        \n",
    "        total_balanced_val_accuracy += balanced_accuracy_score(\n",
    "                                        labels.argmax(axis=1),\n",
    "                                        logits.argmax(axis=1))\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_val_accuracy / len(val_loader)\n",
    "    avg_balanced_val_accuracy = total_balanced_val_accuracy / len(val_loader)\n",
    "    validation_time = time_step(time.time() - t0)\n",
    "    \n",
    "    print(f\"Average validation loss: {avg_val_loss:.2f}\")\n",
    "    print(f\"Average validation accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"Average balanced validation accuracy: {avg_balanced_val_accuracy:.2f}\")\n",
    "    print(f\"Validation time: {validation_time}\\n\")\n",
    "    \n",
    "print(\"Success!\\n\")\n",
    "print(f\"Total training time: {time_step(time.time() - total_t0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
